import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset, WeightedRandomSampler
import timm
from timm.data import Mixup
from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy
from sklearn.model_selection import StratifiedKFold
import numpy as np
import random
from collections import Counter

# Set seeds for reproducibility
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed()

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Hyperparameters
NUM_CLASSES = 100
BATCH_SIZE = 24
MAX_EPOCHS = 50
BASE_LR = 3e-5
WEIGHT_DECAY = 0.05
MIXUP_ALPHA = 0.8
CUTMIX_ALPHA = 1.0
LABEL_SMOOTHING = 0.1
IMAGE_SIZE = 224
ACCUMULATION_STEPS = 4
WARMUP_EPOCHS = 5
DROP_PATH_RATE = 0.1

# Hybrid CNN-ViT Model
class HybridCNNViTModel(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES, dropout=0.3):
        super(HybridCNNViTModel, self).__init__()
        
        self.cnn_backbone = timm.create_model('convnext_small.fb_in22k_ft_in1k', pretrained=True, num_classes=0, drop_path_rate=DROP_PATH_RATE)
        self.vit_backbone = timm.create_model('swin_base_patch4_window7_224.ms_in22k_ft_in1k', pretrained=True, num_classes=0, drop_path_rate=DROP_PATH_RATE)
        
        cnn_features, vit_features = 768, 1024
        self.cnn_reduction = nn.Linear(cnn_features, 512)
        self.vit_reduction = nn.Linear(vit_features, 512)
        self.layer_norm = nn.LayerNorm(1024)
        
        self.fusion = nn.Sequential(
            nn.Linear(1024, 1024), nn.LayerNorm(1024), nn.GELU(), nn.Dropout(dropout)
        )
        self.classifier = nn.Sequential(
            nn.Linear(1024, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(dropout), nn.Linear(512, num_classes)
        )
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)
                    
    def forward(self, x):
        cnn_features = self.cnn_backbone(x).mean(dim=[2, 3])
        vit_features = self.vit_backbone(x)
        cnn_features = self.cnn_reduction(cnn_features)
        vit_features = self.vit_reduction(vit_features)
        combined_features = torch.cat([cnn_features, vit_features], dim=1)
        combined_features = self.layer_norm(combined_features)
        fused_features = self.fusion(combined_features)
        return self.classifier(fused_features)

# Data transforms
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(p=0.1),
    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),
    transforms.RandAugment(num_ops=2, magnitude=9),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.RandomErasing(p=0.25),
])
val_transform = transforms.Compose([
    transforms.Resize(int(IMAGE_SIZE * 1.14)),
    transforms.CenterCrop(IMAGE_SIZE),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Load dataset
data_dir = './competition_data/train_augmented'
full_dataset = datasets.ImageFolder(data_dir, transform=train_transform)
labels = [y for _, y in full_dataset.samples]

# Stratified split
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
train_indices, val_indices = next(skf.split(np.zeros(len(labels)), labels))
train_dataset = Subset(full_dataset, train_indices)
val_dataset = Subset(datasets.ImageFolder(data_dir, transform=val_transform), val_indices)

# Data loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)

# Mixup and CutMix augmentation
mixup_fn = Mixup(mixup_alpha=MIXUP_ALPHA, cutmix_alpha=CUTMIX_ALPHA, prob=1.0, switch_prob=0.5, mode='batch', label_smoothing=LABEL_SMOOTHING, num_classes=NUM_CLASSES)

# Model setup
model = HybridCNNViTModel(num_classes=NUM_CLASSES).to(device)

# Loss functions
mixup_criterion = SoftTargetCrossEntropy().to(device)
criterion = LabelSmoothingCrossEntropy(smoothing=LABEL_SMOOTHING).to(device)

# Optimizer
optimizer = optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)

# Learning rate scheduler
scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=BASE_LR, epochs=MAX_EPOCHS, steps_per_epoch=len(train_loader))

# Training loop placeholder
print("Training loop implementation needed...")



def train_model(model, train_loader, val_loader, optimizer, scheduler, mixup_fn, num_epochs=MAX_EPOCHS, accumulation_steps=ACCUMULATION_STEPS):
    best_val_acc = 0.0
    scaler = torch.cuda.amp.GradScaler()
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        optimizer.zero_grad()
        
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            inputs, targets = inputs.to(device), targets.to(device)
            
            # Apply Mixup augmentation
            if epoch >= 5:  # Start Mixup after a few warmup epochs
                inputs, targets = mixup_fn(inputs, targets)
                use_mixup = True
            else:
                use_mixup = False
            
            with torch.cuda.amp.autocast():
                outputs = model(inputs)
                loss = mixup_criterion(outputs, targets) if use_mixup else criterion(outputs, targets)
                loss = loss / accumulation_steps
            
            scaler.scale(loss).backward()
            
            if (batch_idx + 1) % accumulation_steps == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                scheduler.step()
            
            train_loss += loss.item() * accumulation_steps
            
            if not use_mixup:
                _, predicted = outputs.max(1)
                train_total += targets.size(0)
                train_correct += predicted.eq(targets).sum().item()
            
            if batch_idx % 20 == 0:
                print(f'Epoch: {epoch+1}/{num_epochs} | Batch: {batch_idx}/{len(train_loader)} | Loss: {loss.item() * accumulation_steps:.4f}')
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                
                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += targets.size(0)
                val_correct += predicted.eq(targets).sum().item()
        
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        train_acc = train_correct / train_total if train_total > 0 else 0
        val_acc = val_correct / val_total
        
        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        
        print(f'\nEpoch {epoch+1}/{num_epochs} Summary:')
        print(f'Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f}')
        print(f'Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}')
        
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_hybrid_model.pth')
            print(f'Model saved with Val Acc: {val_acc:.4f}')
        
        print("-" * 60)
    
    return history
